{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Scope of the study: evaluate regeression model by training, validating and testing Linear regression, Random Forest and DNN. \n",
    "\n",
    "\n",
    "Material datasets downloaded with Material Project datasets (MPR) API. Datasets used has Silicon as its primary element. Read dataset as pandas framework. Used matplotlib and pandas plotting function to visulalize histogram distribution, features correlation.\n",
    "\n",
    "\n",
    "Used energy above hull as the parameter of interest (POI). Nine features used to train ML. Applied filters to remove duplicate, undstable, NaN cbm and vbm.\n",
    "\n",
    "\n",
    "**Importing packages from:**\n",
    "\n",
    "*DNN --> tensorflow*\n",
    "\n",
    "*Random Forest & Linear Regression --> sklearn*\n",
    "\n",
    "*Plotting*\n",
    "\n",
    "*MPR*\n",
    "\n",
    "*Other packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as ks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#import tensorflow.contrib.slim as slim\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "from collections import OrderedDict, defaultdict\n",
    "from flatten_dict import flatten\n",
    "import seaborn as sns\n",
    "\n",
    "# material project datasets\n",
    "from mp_api.client.mprester import MPRester \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Information**\n",
    "\n",
    "- Define features\n",
    "\n",
    "- Obtain Data from MPR (Material Project Reader)\n",
    "\n",
    "    - Filter unstable entries, band gap, formation energy\n",
    "\n",
    "- Put the data in pandas datafram \n",
    "\n",
    "- Prepare pd.DataFrame for ML\n",
    "\n",
    "- Add volume and magnetization descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = [\"formation_energy_per_atom\", \"band_gap\", \"energy_per_atom\",\"total_magnetization\",\n",
    "         \"volume\",\"composition\",\"energy_above_hull\" ,\"nelements\",'nsites',\"formula_pretty\",\"is_stable\",\"density\"#]\n",
    "         ,\"vbm\",\"cbm\"]\n",
    "\n",
    "apiid = 'fpVY0sh8FhtuhcJizsr8DROWxQ4AiYYR'\n",
    "\n",
    "\n",
    "\n",
    "with MPRester(apiid) as mpr:\n",
    "    docs = mpr.materials.summary.search(elements=['Si'],#,'O'],\n",
    "                                        #num_elements = (2,10),\n",
    "                                        #band_gap=(0, 10),\n",
    "                                        #energy_above_hull=(0,0.1),\n",
    "                                        #formation_energy=(-20,5),\n",
    "                                        fields=field,\n",
    "                                        #is_stable=True\n",
    "                                        )\n",
    "flattened = [{\n",
    "      k: v\n",
    "      for k, v in flatten(doc.dict(), reducer=\"dot\").items()\n",
    "      if k != \"fields_not_requested\"\n",
    "  } for doc in docs]\n",
    "\n",
    "df = pd.DataFrame.from_records(flattened)\n",
    "print('before removing duplicate size=',df.shape)\n",
    "df.drop_duplicates('formula_pretty', keep='first', inplace=True)\n",
    "df.dropna(subset=[\"cbm\",\"vbm\"],inplace=True) # filter cbm and vbm\n",
    "df  = df[(df['formation_energy_per_atom']>-20) & (df['formation_energy_per_atom']<5)]\n",
    "print('after removing duplicate size=',df.shape)\n",
    "\n",
    "all_columns_inmpr = df.columns\n",
    "df_mp = df.drop(columns=[col for col in df if col not in field])\n",
    "\n",
    "df_mp['vpa'] = df['volume']/df['nsites']\n",
    "df_mp['magmom_pa'] = df['total_magnetization'] / df['nsites']\n",
    "\n",
    "df_mp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1:**\n",
    "\n",
    "- Explore dataset and statistics\n",
    "\n",
    "- Visualize features: histograms and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['energy_above_hull','volume','nsites','is_stable','formula_pretty','total_magnetization']\n",
    "included = [c for c in df_mp.columns if c not in excluded]\n",
    "df_corr = df_mp[included]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df_corr.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.hist(figsize=(12, 8), bins=30, color='skyblue', edgecolor='black')\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Fit Linear Regression model:\n",
    "\n",
    "- Define target and features\n",
    "\n",
    "- Fit Linear Regression model\n",
    "\n",
    "- Plot results\n",
    "\n",
    "- Cross validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_mp['energy_above_hull'].values\n",
    "excluded = ['energy_above_hull','volume','nsites','is_stable','formula_pretty','total_magnetization']\n",
    "included = [c for c in df_mp.columns if c not in excluded]\n",
    "X = df_mp[included].values\n",
    "\n",
    "#print(\"Random Forest features are: {}\".format(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X, y)\n",
    "\n",
    "# get fit statistics\n",
    "print('R2 = ' + str(round(lr.score(X, y), 3)))\n",
    "print('RMSE = %.3f' % np.sqrt(mean_squared_error(y_true=y, y_pred=lr.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr.predict(X),y,'.')\n",
    "plt.plot([0,7],[0,7])\n",
    "plt.xlabel('predict')\n",
    "plt.ylabel('true')\n",
    "plt.title('Linear Regression Model: true vs predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Use 10-fold cross validation (90% training, 10% test)\n",
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# compute cross validation scores for random forest model\n",
    "scores = cross_val_score(lr, X, y, scoring='neg_mean_squared_error',\n",
    "                         cv=crossvalidation, n_jobs=1)\n",
    "rmse_scores = [np.sqrt(abs(s)) for s in scores]\n",
    "\n",
    "print('Cross-validation results:')\n",
    "print('Folds: %i, mean RMSE: %.3f' % (len(scores), np.mean(np.abs(rmse_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression\n",
    "\n",
    "Fit Random Forest regression model:\n",
    "\n",
    "- Define target and features\n",
    "\n",
    "- Fit random forest model\n",
    "\n",
    "- Plot the results\n",
    "\n",
    "- Cross validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=1)\n",
    "\n",
    "rf.fit(X, y)\n",
    "print('R2 = ' + str(round(rf.score(X, y), 3)))\n",
    "print('RMSE = %.3f' % np.sqrt(mean_squared_error(y_true=y, y_pred=rf.predict(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rf.predict(X), y,'.')\n",
    "plt.plot([0,7],[0,7])\n",
    "plt.xlabel('predict')\n",
    "plt.ylabel('true')\n",
    "plt.title(\"Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf, X, y, scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=1)\n",
    "\n",
    "rmse_scores = [np.sqrt(abs(s)) for s in scores]\n",
    "print('Cross-validation results:')\n",
    "print('Folds: %i, mean RMSE: %.3f' % (len(scores), np.mean(np.abs(rmse_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "included = np.asarray(included)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.bar(included[indices],height=importances[indices])\n",
    "plt.tick_params(axis='x', rotation=80)\n",
    "plt.yscale('log')\n",
    "plt.title('Random Forest Feature importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN:\n",
    "\n",
    "- Used same dataset features to train, validate and test DNN; used 70% to train, and 15% each to validate and test\n",
    "\n",
    "- Used 14 dense layers with 1024 neurons (units)\n",
    "\n",
    "- Activation function: ReLU (rectified linear units)\n",
    "\n",
    "- Dropout 30% of neuorons to prevent overfitting\n",
    "\n",
    "- Batch normalization (layer that normalizes input batch to its mean and std) for each dense layer\n",
    "\n",
    "- early stopping applied to avoid overfitting\n",
    "\n",
    "- Adam optimizer used with learning rate = 0.0003 (to avoid overfitting), loss function \"mae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_mp['energy_above_hull'].values\n",
    "excluded = ['energy_above_hull','volume','nsites','is_stable','formula_pretty','total_magnetization']\n",
    "included = [c for c in df_mp.columns if c not in excluded]\n",
    "X = df_mp[included].values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp,y_train, y_temp = train_test_split(X,y, test_size=0.3, random_state=1234567)\n",
    "X_val,   X_test,y_val,   y_test = train_test_split(X_temp,y_temp,test_size=0.5, random_state=1234567)\n",
    "\n",
    "#X_train.shape, X_valid.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=[9]),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam,\n",
    "    loss='mae',\n",
    ")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience = 5,\n",
    "    min_delta = 0.001,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch = 256\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=batch,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['loss'][3:].plot()\n",
    "history_df['val_loss'][3:].plot()\n",
    "plt.legend(['loss','val_loss'])\n",
    "plt.yscale('log')\n",
    "#history_df['accuracy'].plot()\n",
    "#history_df['val_accuracy'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test, y_test)\n",
    "#print(f\"Test Accuracy: {test_acc}, Test loss: {test_loss}\")\n",
    "print('test loss: {test_loss}')\n",
    "\n",
    "ydnn_pred = model.predict(X_test)\n",
    "plt.plot(ydnn_pred,y_test,'o')\n",
    "plt.plot([0,7],[0,7])\n",
    "plt.xlabel(\"predict\")\n",
    "plt.ylabel(\"true\")\n",
    "plt.title(\"DNN energy above hull: true vs prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "#data = load_diabetes()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "important_features = xgb_model.feature_importances_\n",
    "selected_features = X_train[:, important_features > 0.01]  # Filtering based on importance\n",
    "\n",
    "# Define DNN model\n",
    "dnn_model = Sequential([\n",
    "    Dense(64, activation=\"relu\", input_shape=(selected_features.shape[1],)),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(1)  # Regression output\n",
    "])\n",
    "\n",
    "# Compile & train\n",
    "dnn_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "dnn_model.fit(selected_features, y_train, epochs=10, batch_size=16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
